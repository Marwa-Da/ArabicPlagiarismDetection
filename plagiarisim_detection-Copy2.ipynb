{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "بدايةً يتوجب علينا تنظيف الملف من المحارف الغير عربية والأرقام وعلامات الترقيم باستثناء(.!؟) وذلك لنتمكن من تقسيم الملف إلى جمل في المرحلة التالية. وبعد ذلك علينا إزالة كلمات التوقف والقيام بالتجذيع. من ثم نقوم بحساب تكرار كل كلمة على صعيد النص وتثقيل كل جملة من جمل النص من خلال تابع المتوسط الحسابي لكلمات الجملة .المرحلة النهائية سيتم شرحها في وقتها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\python27\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import codecs \n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize,sent_tokenize, WhitespaceTokenizer\n",
    "import imp,os\n",
    "import nltk\n",
    "import math\n",
    "import sys\n",
    "import psycopg2\n",
    "from psycopg2 import connect\n",
    "from psycopg2 import extras\n",
    "\n",
    "tp = imp.load_source('textpro', 'textpro.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x202b;هنا يتم التعامل مع الملفات المشبوهة  <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split text to sentences\n",
    "def tokzr_SENT(_txt): \n",
    "    return ( re.findall(ur'(?ms)\\s*(.*?(?:\\.|؟|!)+)', _txt))  # split sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the suspicious file name and path :\n",
      "file name :  suspicious-document330.txt\n",
      "path :  F:\\ITE\\.5th year\\1\\NLP\\code fragments\n"
     ]
    }
   ],
   "source": [
    "print \"Enter the suspicious file name and path :\"\n",
    "file_name = raw_input(\"file name :  \")\n",
    "path = raw_input(\"path :  \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_suspicious_files = os.listdir(\"suspicious-documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_stemmed_sus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "<FreqDist with 276 samples and 342 outcomes>\n"
     ]
    }
   ],
   "source": [
    "if file_name in all_suspicious_files:    # إن كان الملف واحد من ملفات قاعدة المعطيات\n",
    "    print \" b\"\n",
    "    m = filter(None, re.split(r'(\\w.+)(\\.\\w+)',file_name ))[0]\n",
    "    \n",
    "    con = None\n",
    "    con = connect(dbname='plagiarisimdb2', user='postgres', host='localhost', password='ucv')\n",
    "    cursor = con.cursor('cursor_unique_name', cursor_factory=psycopg2.extras.DictCursor)\n",
    "    # execute the Query\n",
    "    query = 'SELECT cleaned_sentence FROM public.suspicious_sentence WHERE suspicious_file_name = \\''+ m + \"\\'\"\n",
    "    cursor.execute(query)\n",
    "    _sentences = cursor.fetchall()\n",
    "    for s in _sentences: \n",
    "        print s\n",
    "        for mm in s.values():\n",
    "            print tp.lightStemAr(mm)\n",
    "            #tok.append(tp.lightStemAr(mm))  # stemmed sentence\n",
    "    \n",
    "    #...\n",
    "    cursor = con.cursor()\n",
    "    query = 'SELECT count(*) FROM public.suspicious_sentence WHERE suspicious_file_name = \\''+ m + \"\\'\"\n",
    "    cursor.execute(query)\n",
    "    file_length = cursor.fetchall()\n",
    "    \n",
    "\n",
    "else:    #إن لم يكن الملف في قاعدة المعطيات للملفات المشبوهة\n",
    "    print \"o\"\n",
    "    with codecs.open(path + \"\\\\\" + file_name ,'r',encoding = 'utf-8') as f: \n",
    "        txt = f.read()  #... 1 file\n",
    "    clean_text = tp.process_text_Marwa(text=txt,removePunct=True,removeSW=True,removeNum=True)\n",
    "    # string\n",
    "    clean_text2 = \" \".join(clean_text)\n",
    "    sentences = tokzr_SENT(clean_text2)\n",
    "    # stemmed sentence (and removed . ! ?)\n",
    "    # sentence is string\n",
    "\n",
    "    text_stemmed_word_tokens= []\n",
    "    tokenized_sentences_length = {}  # dictionary for tokenized sentence with its length\n",
    "    i =0\n",
    "    file_length = len(sentences)\n",
    "    for s in sentences: \n",
    "        _sentences = tp.process_text(text = s, removePunct=True,removeSW=False,removeNum=False)   # sentence without .!?\n",
    "        \n",
    "        stemmed_sentences = tp.lightStemAr(word_list= _sentences)  # stemmed sentence\n",
    "        #tok = WhitespaceTokenizer().tokenize(_sentences) #  sentence to words\n",
    "        tok = _sentences\n",
    "        tokenized_sentences_length.update({i:[0,len(tok),tok]})\n",
    "        i+= 1\n",
    "        text_stemmed_word_tokens.extend(tok) # list of all text (stemmed)words\n",
    "        list_stemmed_sus.extend(stemmed_sentences)\n",
    "    #print file_length\n",
    "\n",
    "fdist = nltk.FreqDist(text_stemmed_word_tokens)\n",
    "print fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for k in fdist: \n",
    "#    print fdist[k]\n",
    "\n",
    "print len(text_stemmed_word_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "من أجل كل جملة:<br/>\n",
    "من اجل كل كلمة:<br/>\n",
    "إحضار freqDist <br/>\n",
    "جمع كل التكرارات لهذه الجملة <br/>\n",
    "التقسيم على عددهم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.0\n",
      "1.939 83.0\n",
      "2.184 94.0\n",
      "2.41 25.0\n",
      "1.25 55.0\n",
      "1.774 82.0\n",
      "2.733 20.0\n",
      "1.25 25.0\n",
      "1.042 58.0\n",
      "1.758 36.0\n",
      "1.565 96.0\n",
      "1.745\n"
     ]
    }
   ],
   "source": [
    "q =0\n",
    "\n",
    "while q<file_length: \n",
    "    \n",
    "    _list = tokenized_sentences_length[q]  # list of 3 components\n",
    "    frqDist_aSentence = [fdist[unicode(word)] for word in _list[2]]  #أخذت ليستة التوكينز ثم جبت التكرار لكل وحدة\n",
    "    \n",
    "    #for word in _list[2]: \n",
    "    #    print fdist[unicode(word)],word\n",
    "    \n",
    "    t = math.fsum(frqDist_aSentence)\n",
    "    print t\n",
    "    tt = t/_list[1]\n",
    "    _list[0] = float(\"%.3f\" %tt)\n",
    "    print _list[0],\n",
    "    q+= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bigram,trigram نأتي الآن لمرحلة توليد الاستعلامات: سنختار أول 3 جمل وزنها كبييير ثم نشكل من كل واحدة منها جميع التشكيلات الممكنة ل "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.733, 5), (2.41, 2), (2.184, 1), (1.939, 0), (1.774, 4), (1.758, 8), (1.745, 10), (1.565, 9), (1.25, 6), (1.25, 3), (1.042, 7)]\n",
      "[u'\\u0648\\u0623\\u0634\\u0627\\u0631', u'\\u0627\\u0644\\u062c\\u0647\\u0646\\u064a', u'\\u0627\\u0644\\u0633\\u0639\\u0648\\u062f\\u064a\\u0629', u'\\u0646\\u062c\\u062d\\u062a', u'\\u0639\\u0628\\u0631', u'\\u0628\\u0631\\u0646\\u0627\\u0645\\u062c', u'\\u0627\\u0643\\u062a\\u0634\\u0641', u'\\u0627\\u0644\\u0645\\u0645\\u0644\\u0643\\u0629', u'\\u0627\\u0633\\u062a\\u0642\\u0637\\u0627\\u0628', u'\\u0645\\u0626\\u0627\\u062a', u'\\u0627\\u0644\\u0633\\u064a\\u0627\\u062d', u'\\u0627\\u0644\\u064a\\u0627\\u0628\\u0627\\u0646\\u064a\\u064a\\u0646', u'\\u0633\\u0646\\u0648\\u064a\\u0627', u'\\u0628\\u062f\\u0623', u'\\u062a\\u0646\\u0638\\u064a\\u0645', u'\\u0627\\u0644\\u0628\\u0631\\u0646\\u0627\\u0645\\u062c', u'\\u0645\\u0646\\u062a\\u0635\\u0641', u'\\u0642\\u0627\\u0645\\u062a', u'\\u0628\\u062a\\u0633\\u064a\\u064a\\u0631', u'\\u0631\\u062d\\u0644\\u0627\\u062a', u'\\u062e\\u0627\\u0635\\u0629', u'\\u0645\\u0628\\u0627\\u0634\\u0631\\u0629', u'\\u0627\\u0644\\u064a\\u0627\\u0628\\u0627\\u0646', u'\\u0627\\u0644\\u0645\\u0645\\u0644\\u0643\\u0629', u'\\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629', u'\\u0627\\u0644\\u0633\\u0639\\u0648\\u062f\\u064a\\u0629', u'\\u0646\\u0642\\u0644\\u062a', u'\\u0639\\u0628\\u0631\\u0647\\u0627', u'\\u0633\\u0627\\u0626\\u062d\\u0627', u'\\u064a\\u0627\\u0628\\u0627\\u0646\\u064a\\u0627']\n"
     ]
    }
   ],
   "source": [
    "# descending sort of sentences' weight\n",
    "\n",
    "s_list = [(tokenized_sentences_length[key][0], key) for key in tokenized_sentences_length]\n",
    "s_list.sort(reverse=True)\n",
    "print s_list\n",
    "\n",
    "Max_weight_sent_list = []\n",
    "\n",
    "# we will choose the first 3 sentences >>> they are tokenized so ill work immediately for  bi&tri -gram\n",
    "Max_weight_sent_list.append(tokenized_sentences_length[s_list[0][1]][2])\n",
    "Max_weight_sent_list.append(tokenized_sentences_length[s_list[1][1]][2])\n",
    "Max_weight_sent_list.append(tokenized_sentences_length[s_list[2][1]][2])\n",
    "\n",
    "print Max_weight_sent_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generating bigrams and trigrams \n",
    "\n",
    "Query_list1 = []\n",
    "Query_list2 = []\n",
    "\n",
    "# fill the query list first with every sentence bigrams & trigrams\n",
    "for sent in Max_weight_sent_list: \n",
    "    Query_list1.extend(list(nltk.bigrams(sent)))\n",
    "    Query_list2.extend(list(nltk.trigrams(sent)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وأشار الجهني\n",
      "الجهني السعودية\n",
      "السعودية نجحت\n",
      "نجحت عبر\n",
      "عبر برنامج\n",
      "برنامج اكتشف\n",
      "اكتشف المملكة\n",
      "المملكة استقطاب\n",
      "استقطاب مئات\n",
      "مئات السياح\n",
      "السياح اليابانيين\n",
      "اليابانيين سنويا\n",
      "سنويا بدأ\n",
      "بدأ تنظيم\n",
      "تنظيم البرنامج\n",
      "البرنامج منتصف\n",
      "منتصف قامت\n",
      "قامت بتسيير\n",
      "بتسيير رحلات\n",
      "رحلات خاصة\n",
      "خاصة مباشرة\n",
      "مباشرة اليابان\n",
      "اليابان المملكة\n",
      "المملكة العربية\n",
      "العربية السعودية\n",
      "السعودية نقلت\n",
      "نقلت عبرها\n",
      "عبرها سائحا\n",
      "سائحا يابانيا\n"
     ]
    }
   ],
   "source": [
    "Query_list3= []\n",
    "Query_list3.extend(list(nltk.bigrams(Max_weight_sent_list[0])))\n",
    "for i in Query_list3: \n",
    "    print i[0],i[1]#,i[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, STORED\n",
    "\n",
    "schema = Schema(title=TEXT(stored=True), content=TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "معلومات مهمة\n",
    "\n",
    "whoosh.fields.STORED\n",
    "This field is stored with the document, but not indexed. This field type is not indexed and not searchable. This is useful for document information you want to display to the user in the search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whoosh.fields.TEXT\n",
    "This type is for body text. It indexes (and optionally stores) the text and stores term positions to allow phrase searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import codecs\n",
    "import re\n",
    "from whoosh.index import create_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"index\"):\n",
    "    os.mkdir(\"index\")\n",
    "ix = create_in(\"index\", schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "بعد إنشاء الفهرس أصبح بإمكاني فتحه :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.index import open_dir\n",
    "\n",
    "ix = open_dir(\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************\n",
    "\n",
    "يساعدني كاتب الفهرس على إضافة مستندات جديدة ويجب أن نعلم أن إضافة المستندات للفهرس هي عملية يدوية بحتةةةةةة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = ix.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_folder = 'source-documents'\n",
    "all_source_files = os.listdir(source_folder)\n",
    "for file_namee in all_source_files:\n",
    "    with codecs.open(source_folder + \"\\\\\" + file_namee,'r',encoding = 'utf-8') as f: \n",
    "        txt = f.read() \n",
    "        tit = filter(None, re.split(r'(\\w.+)(\\.\\w+)',file_namee ))[0]\n",
    "        writer.add_document(title=unicode(tit), content=unicode(txt))\n",
    "        \n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "parser = QueryParser(\"content\", ix.schema)\n",
    "#query = u\"الخطوط الجوية العربية\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse a query string\n",
    "'''\n",
    "from nltk import ngrams\n",
    "n = 5\n",
    "news = \" \".join(Max_weight_sent_list[0])\n",
    "fourth_grams = ngrams(news.split(), n)\n",
    "'''\n",
    "\n",
    "result = set()\n",
    "for gram in Query_list3: \n",
    "    query = \" \".join(gram)   \n",
    "    #print \"1\"\n",
    "    myquery = parser.parse(query)\n",
    "    with ix.searcher() as searcher:\n",
    "        # Use terms=True to record term matches for each hit\n",
    "        results = searcher.search(myquery, terms=True,limit = 50)\n",
    "        for hit in results: \n",
    "            #print hit.fields()\n",
    "            result.add(hit.fields()[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'print \"Opening the file...\"            \\nwith open(\"candidate files.txt\", \\'w\\' )as target: \\n    for m in result: \\n        target.write(m)\\n        target.write(\"\\n\")\\n    target.write(path + \"\\\\\" + file_name)\\nprint \"fine\"\\n\\nwith open(\"candidate files\",\\'r\\') as f:\\n    for line in f: \\n        print line\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''print \"Opening the file...\"            \n",
    "with open(\"candidate files.txt\", 'w' )as target: \n",
    "    for m in result: \n",
    "        target.write(m)\n",
    "        target.write(\"\\n\")\n",
    "    target.write(path + \"\\\\\" + file_name)\n",
    "print \"fine\"\n",
    "\n",
    "with open(\"candidate files\",'r') as f:\n",
    "    for line in f: \n",
    "        print line\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "ttt\n"
     ]
    }
   ],
   "source": [
    "value = u\"source-document00354\" \n",
    "if value in result: \n",
    "    print \"t\"\n",
    "value = u\"source-document00362\" \n",
    "if value in result: \n",
    "    print \"ttt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import psycopg2\n",
    "from psycopg2 import connect\n",
    "from psycopg2 import extras\n",
    "\n",
    "from scipy import linalg, mat, dot\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "con = None\n",
    "con = connect(dbname=\"plagiarisimdb2\", user='postgres', host='localhost', password='ucv')\n",
    "#cursor = con.cursor('cursor_unique_name', cursor_factory=psycopg2.extras.DictCursor)\n",
    "\n",
    "for i in result: \n",
    "# execute the Query\n",
    "    cursor = con.cursor()\n",
    "    query = 'SELECT stemmed_sentence FROM public.source_sentence WHERE source_file_name = \\''+ i + \"\\'\"\n",
    "    cursor.execute(query)\n",
    "    _sentences = cursor.fetchall()\n",
    "    # forming trigrams\n",
    "    for file1_sent in list_stemmed_sus: \n",
    "        sent1_trigram = list(nltk.trigrams(file1_sent))\n",
    "        sent1_counter=collections.Counter(sent1_trigram)_\n",
    "        trigram_vector1 = {}     # vector of all trigrams frequncy in sentence 1\n",
    "        for file2_sent in _sentences: \n",
    "            sent2_trigram = list(nltk.trigrams(file2_sent))\n",
    "            sent2_counter=collections.Counter(sent2_trigram)\n",
    "            all\n",
    "            all_trigram_vector2 = {}     # vector of all trigrams frequncy in sentence 2\n",
    "\n",
    "            all_trigram_vector1 = dict(sent1_counter.items() + sent2_counter.items() +\n",
    "                          [(k, sent1_counter[k] + sent2_counter[k]) for k in set(sent2_counter) & set(sent1_counter)] + [(k, 0) for k in set(sent2_counter) if k not in set(sent1_counter) ])\n",
    "\n",
    "            all_trigram_vector2 = dict(sent1_counter.items() + sent2_counter.items() +\n",
    "                          [(k, sent1_counter[k] + sent2_counter[k]) for k in set(sent2_counter) & set(sent1_counter)] + [(k, 0) for k in set(sent1_counter) if k not in set(sent2_counter)])\n",
    "            \n",
    "            # creating trigram vector for each sentence:\n",
    "            matrix = mat( [all_trigram_vector1.values(),all_trigram_vector2.values()] )\n",
    "            cosine_similarity =  dot(matrix[0],matrix[1].T)/np.linalg.norm(matrix[0])/np.linalg.norm(matrix[1])\n",
    "            cc=0\n",
    "            count =0\n",
    "            boole = False\n",
    "            \n",
    "            if (cosine_similarity > 0.25): \n",
    "                for cc in all_trigram_vector1: \n",
    "                    if (all_trigram_vector1[cc] != 0 & all_trigram_vector2[cc] !=0): \n",
    "                        count+= 1\n",
    "                    if(count == 3): \n",
    "                        boole = True\n",
    "                        break\n",
    "            if boole : \n",
    "                print \" 2 similar sentences\"\n",
    "                print file1_sent\n",
    "                print file2_sent\n",
    "                print \"#####################\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "print len(_sentences)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:python27]",
   "language": "python",
   "name": "conda-env-python27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
